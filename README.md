## 时间安排
|    日期  |    主题  |   课程PPT  |  阅读材料  |  任务 | Deadline |
|---------|---------|---------|---------|---------|---------|
| 4.14  |  [直播]开课典礼 & 课程介绍 & NLP概论  |[课程内容](http://47.94.6.102/NLPCamp3/Lesson1-Intro_Complexity) |[github 教学视频](http://www.greedyai.com/course/46) | [Homework0](http://47.94.6.102/NLPCamp3/Homework0)  |Due on 4/16|
| 4.17  |  [录播]问答系统（1） - 深入浅出必备基础：算法复杂度和动态规划 || [时间复杂度](https://www.jianshu.com/p/f4cca5ce055a)<br/> [edit distance](https://algorithms.tutorialhorizon.com/dynamic-programming-edit-distance-problem/)<br/>[Master's Theorem](http://people.csail.mit.edu/thies/6.046-web/master.pdf)<br/>| | |
| 4.19  |  [录播]问答系统（2） - 分词，拼写纠错，停用词过滤，词的标准化，词袋模型，文本相似度计算 | |[文本预处理（代码参考）](https://www.kaggle.com/shashanksai/text-preprocessing-using-python)<br/>[分词中的最大匹配算法](https://blog.csdn.net/selinda001/article/details/79345072)<br/> [拼写纠错](https://web.stanford.edu/class/cs124/lec/spelling.pdf)<br/>[Edit Distance](https://www.geeksforgeeks.org/edit-distance-dp-5/)<br/>[DP练习题](https://people.cs.clemson.edu/~bcdean/dp_practice/)<br/>[Porter Stemmer](https://tartarus.org/martin/PorterStemmer/java.txt)<br/>[tf-idf介绍（技术博客）](https://www.cnblogs.com/pinard/p/6693230.html)[Porter Stemming （网页版介绍）]( http://facweb.cs.depaul.edu/mobasher/classes/csc575/papers/porter-algorithm.html)<br/><h4>高级论文阅读</h4>[QuAC : Question Answering in Context](https://arxiv.org/pdf/1808.07036.pdf)<br/>[Coarse-to-Fine Question Answering for Long Documents](https://homes.cs.washington.edu/~eunsol/papers/acl17eunsol.pdf)<br/>[QA workshop](https://mrqa2018.github.io/) |[Project1 Due on 5/11](http://47.94.6.102/NLPCamp3/Project1)||
| 4.20  |  [Review]基于几个实例书写动态规划（郭老师）||||
| 4.21  |  [直播]问答系统（3） - 词向量，句子向量，倒排表，项目作业的解释 |[课程内容](http://47.94.6.102/NLPCamp3/Lesson4-TextProcessing)  |[倒排列表（Manning et al. 第一章）](https://nlp.stanford.edu/IR-book/pdf/01bool.pdf)<br/>[余弦相似度介绍（技术博客）](https://blog.csdn.net/ifnoelse/article/details/7766123)<br/><br/>[From Word Embeddings To Document Distances（计算语句相似度，ICML 2015）](http://proceedings.mlr.press/v37/kusnerb15.pdf)<br/>[Optimizing Chinese Word Segmentation for Machine Translation Performance（分词，ACL 2008）](https://nlp.stanford.edu/pubs/acl-wmt08-cws.pdf)|||
| 4.24  |  [录播]语言模型（1）- Ngram, 概率预测 |[课程内容](http://47.94.6.102/NLPCamp3/Lesson5-6)|<h4>高级阅读</h4>[A Neural Probabilistic Language Model](http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)<br/><br/> [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)|||
| 4.26  |  [录播]语言模型（2）- Smoothing |[课程内容](http://47.94.6.102/NLPCamp3/Lesson5-6)|||
| 4.27  |  [Review]相似度计算（姜老师）||||
| 4.28  |  [直播] 词性标注实战，维特比算法 |[课程内容](http://47.94.6.102/NLPCamp3/Lesson7)|||
| 5.1   |  [录播] 语言模型（3）-Smoothing, 专家系统与朴素贝叶斯 ||[An Empirical Study of Smoothing Techniques for Language Modeling 1996](https://aclweb.org/anthology/P96-1041)<br/>||
| 5.5   |  [直播] 逻辑回归介绍与推导（面试必备！） ||[30 Questions to test your understanding of Logistic Regression](https://www.analyticsvidhya.com/blog/2017/08/skilltest-logistic-regression/)<br/>[Convex Functions, Gradient Descent, Convergence Rates by UBC](https://www.cs.ubc.ca/~schmidtm/Courses/540-W16/L4.pdf)<br/>||
| 5.8   |  [录播] 过拟合，正则，L1,L2的应用（面试必备！） ||[Regularization and Variable Selection via the Elastic Net](https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&%20Hastie.pdf)<br/>[A note on the group lasso and a sparse group lasso](https://arxiv.org/abs/1001.0736)<br/>||
| 5.10  |  [录播] 交叉验证，MLE VS MAP, LASSO， Coordinate Descent （面试必备！） ||[机器学习中的MLE、MAP、贝叶斯估计 知乎](https://zhuanlan.zhihu.com/p/37215276)<br/>[聊一聊机器学习的MLE和MAP：最大似然估计和最大后验估计](https://zhuanlan.zhihu.com/p/32480810)<br/>[贝叶斯思想以及与最大似然估计、最大后验估计的区别 CSDN](https://blog.csdn.net/ljp812184246/article/details/51176227)<br/>[Coordinate descent algorithms for lasso penalized regression 2008](https://arxiv.org/pdf/0803.3876.pdf)<br/>[LEAST ANGLE REGRESSION 2004](https://arxiv.org/pdf/math/0406456.pdf)<br/>[Parallel Coordinate Descent for L1-Regularized Loss Minimization 2011](https://arxiv.org/abs/1105.5379)<br/>|||
| 5.11  |  [Review]问答系统讲解（胡老师）||||
| 5.12  |  [直播] 浅谈凸优化, Capstone项目介绍 |[课程内容](http://47.94.6.102/NLPCamp3/Optimization)|[Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)<br/>[Introduction to Convex Optimization for Machine Learning Slides by John Duchi](https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/lectures/optimization/slides.pdf)<br/>[Convex Optimization Book Slides](https://web.stanford.edu/~boyd/cvxbook/bv_cvxslides.pdf)<br/>[Convex Optimization Book](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)<br/>|[Project 2](http://47.94.6.102/NLPCamp3/Project2) out|
| 5.15  |  [录播] SVM详解-1 （面试必备！） ||[A Tutorial on Support Vector Machines for Pattern Recognition 1998](https://link.springer.com/content/pdf/10.1023/A:1009715923555.pdf)<br/>[A Practical Guide to Support Vector Classification 2016](https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf)<br/>||
| 5.17  |  [录播] SVM详解-2（面试必备！） ||[Support Vector Machines — Kernels and the Kernel Trick](https://cogsys.uni-bamberg.de/teaching/ss06/hs_svm/slides/SVM_Seminarbericht_Hofmann.pdf)<br/>[KERNEL METHODS IN MACHINE LEARNING1](http://www.kernel-machines.org/publications/pdfs/0701907.pdf)<br/>[Kernel Methods for Deep Learning](https://cseweb.ucsd.edu/~saul/papers/nips09_kernel.pdf)<br/>||
| 5.18  |  [Review]文本分类系统（文本分类系统）||||
| 5.19  |  [直播] 信息抽取核心技术，命名实体识别 |[课程代码](http://47.94.6.102/NLPCamp3/IR-NER)|[Mining Knowledge from Text Using Information Extraction 2005](https://www.cs.utexas.edu/~ml/papers/text-kddexplore-05.pdf)<br/>[Unsupervised Models for Named Entity Classification](https://www.aclweb.org/anthology/W99-0613)<br/>[Bidirectional LSTM-CRF Models for Sequence Tagging 2015](https://arxiv.org/abs/1508.01991)<br/>[Accurate Information Extraction from Research Papers using Conditional Random Fields 2004](https://www.aclweb.org/anthology/N04-1042)||
| 5.22  |  [录播] 关系抽取技术- 基于规则，基于监督学习 ||[A SURVEY ON RELATION EXTRACTION](http://www.cs.cmu.edu/~nbach/papers/A-survey-on-Relation-Extraction-Slides.pdf)<br/>[Relation Extraction](https://courses.cs.washington.edu/courses/cse517/13wi/slides/cse517wi13-RelationExtraction.pdf)<br/>||
| 5.24  |  [录播] 关系抽取技术 - Distant Supervision, 实体消歧，实体统一，指代消解||[Distant supervision for relation extraction without labeled data](https://www.aclweb.org/anthology/P09-1113)<br/>[Open information extraction from the web.](https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-429.pdf)<br/>[Web-Scale Information Extraction in KnowItAll ](http://turing.cs.washington.edu/papers/www-paper.pdf)<br/>[TextRunner: Open Information Extraction on the Web](https://tianjun.me/static/essay_resources/RelationExtraction/Paper/p25-yates.pdf)<br/>[Extracting Patterns and Relations from the World Wide Web](http://ilpubs.stanford.edu:8090/421/1/1999-65.pdf)<br/>||
| 5.25  |  [录播] pytorch用法（姜老师）||||
| 5.27  |  [直播] 关系抽取技术 - Bootstrap方法, Snowball ||[Snowball: Extracting Relations from Large Plain-Text Collections](http://www.cs.columbia.edu/~gravano/Papers/2000/dl00.pdf)<br/>|[Project 2](http://47.94.6.102/NLPCamp3/Project2) Due|||
| 5.29  |  [录播] 句法分析， PCFG||[A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition ](https://www.robots.ox.ac.uk/~vgg/rg/papers/hmm.pdf)<br/>[Hidden Markov Models](https://web.stanford.edu/~jurafsky/slp3/A.pdf)<br/>[如何用简单易懂的例子解释隐马尔可夫模型？](https://www.zhihu.com/question/20962240/answer/33561657)<br/>||
| 5.31  |  [录播] 时序模型，HMM介绍||[A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition ](https://www.robots.ox.ac.uk/~vgg/rg/papers/hmm.pdf)<br/>[Hidden Markov Models](https://web.stanford.edu/~jurafsky/slp3/A.pdf)<br/>[如何用简单易懂的例子解释隐马尔可夫模型？](https://www.zhihu.com/question/20962240/answer/33561657)<br/>| [技术文章编写指南&要求](http://47.94.6.102/NLPCamp3/course-info/wikis/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97&%E8%A6%81%E6%B1%82)|
| 6.5   |  [Review]动态规划|[代码](http://47.94.6.102/NLPCamp3/dynamicprogramming)|[Searching and Mining Trillions of Time Series Subsequences under Dynamic Time Warping](https://www.cs.ucr.edu/~eamonn/SIGKDD_trillion.pdf)||
| 6.6   |  [Review]梯度下降和线性回归||||
| 6.12  |  [录播]  HMM的参数估计||[怎么通俗易懂地解释EM算法并且举个例子?修改](https://www.zhihu.com/question/27976634)<br/>[Maximum Likelihood from Incomplete Data via the EM Algorithm](http://web.mit.edu/6.435/www/Dempster77.pdf)<br/>[Clustering, K-Means, EM Tutorial ](https://www.cs.toronto.edu/~jlucas/teaching/csc411/lectures/tut8_handout.pdf)<br/>||
| 6.13  |  [Review] 正则的使用（regularization and overfitting with L1 and L2）|[课程代码](https://github.com/AryeYellow/PyProjects/blob/master/DataScience/Regularization%20with%20L1%20and%20L2.ipynb)|||
| 6.15  |  [Review] Web-Scale Information Extraction in KnowItAll |[课程内容](http://47.94.6.102/NLPCamp3/course-info/blob/master/%E8%AF%BE%E4%BB%B6/review%20session_20190615.pptx)|||
| 6.15  |  [Review] Kernel Trick |||[Project 3](http://47.94.6.102/NLPCamp3/Project3)|
| 6.16  |  [直播] EM算法讲解||[The Expectation Maximization Algorithm A short tutorial](http://www.seanborman.com/publications/EM_algorithm.pdf)||
| 6.19  |  [录播] CRF相关|Log-linear Model介绍，CRF的参数估计|[Log-linear models and conditional random fields](http://cseweb.ucsd.edu/~elkan/250B/CRFs.pdf)|
| 6.22  |  [Review]利用CRF模型做命名实体识别（郭老师）||||
| 6.22  |  [Review]基于语料库训练Glove词向量模型（胡老师）|[课程内容](http://47.94.6.102/NLPCamp3/course-info/blob/master/%E8%AF%BE%E4%BB%B6/GloVe.pdf)|||
| 6.23  |  [Review]HMM参数估计代码讲解（洪老师）|[课程代码](https://github.com/AryeYellow/PyProjects/blob/master/NLP/Viterbi-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/HMM.ipynb)|||
| 6.23  |  [直播] 词向量|分布式表示，SkipGram以及Derivation,|[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)</br>[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)|
| 6.26  |  [录播] 训练词向量， negative sampling |其他词向量算法|[GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)<br/>[Word Representations via Gaussian Embedding](https://arxiv.org/abs/1412.6623)||
| 6.27  |  [Review] 词向量应用 ||||
| 6.29  |  [Review] pytorch实现skip-gram（胡老师）|[课程内容](http://47.94.6.102/NLPCamp3/course-info/blob/master/%E8%AF%BE%E4%BB%B6/skip_gram_code_hpp(2).pdf)|||
| 6.29  |  [Review]Airbnb（KDD 2018 best paper）讲解（郭老师）|[课程内容](http://47.94.6.102/NLPCamp3/course-info/blob/master/%E8%AF%BE%E4%BB%B6/20190629_airbnb.pptx)|||
| 6.30  |  [直播] 词向量总结，ELMO, Subword Model |[课程内容](http://47.94.6.102/NLPCamp3/Lesson0630)|[Project 3] (http://47.94.6.102/NLPCamp3/Project3) Due|
| 7.3   |  [录播] 神经网络||||
| 7.4   |  [Review] 神经网络代码|[课程代码](http://47.94.6.102/NLPCamp3/course-info/blob/master/%E8%AF%BE%E4%BB%B6/neural%20network.ipynb)|||
| 7.6   |  [Review] Project3讲解||||
| 7.7   |  [直播] BP算法推导，代码|[课程内容](http://47.94.6.102/NLPCamp3/Lecture-BackPropagation)|||
| 7.10  |  [录播] 深度学习的训练，RNN模型 ||http://karpathy.github.io/2015/05/21/rnn-effectiveness/,[论文](http://47.94.6.102/NLPCamp3/course-info/blob/master/%E8%AF%BE%E4%BB%B6/visualizing%20and%20understanding%20neural%20models%20in%20nlp.pdf)||
| 7.13  |  [Review] Tensorflow的使用 |[课程内容](http://47.94.6.102/NLPCamp3/course-info/blob/master/%E8%AF%BE%E4%BB%B6/tensorflow_code.ipynb)|||
| 7.13  |  [Review] 使用 LSTM-CRF来做实体识别 |[课程资料](http://47.94.6.102/NLPCamp3/course-info/blob/master/%E8%AF%BE%E4%BB%B6/lstm-crf_application_review%20session_20190713.pptx)|||
| 7.14  |  [直播] Seq2Seq, Beam Search |[课程资料](http://47.94.6.102/NLPCamp3/Lecture-BackPropagation)|||
| 7.21  |  [Review] Seq2Seq实战案例 |[课程资料](http://47.94.6.102/NLPCamp3/course-info/blob/master/%E8%AF%BE%E4%BB%B6/20190629_airbnb.pptx)|||
| 7.21  |  [Review] 利用LSTM生成文本案例 |[课程资料](https://github.com/AryeYellow/NLP/blob/master/TextGeneration/tg_trigram_and_cnn.ipynb)|||
| 7.21  |  [直播] 注意力机制 ||||
| 7.24  |  [直播] Self-Attention与Transformer ||||
| 7.21  |  [直播]  Seq2Seq+Attention实战：聊天机器人 ||||
| 8.1   |  [直播]  Bert |[代码](http://47.94.6.102/NLPCamp3/BERT)|[The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/)||
| 8.4   |  [直播]  Bert（2） |[课程资料](http://47.94.6.102/NLPCamp3/BERT)|||
| 8.4   |  [Review]  Bert（2） |[课程资料](http://47.94.6.102/NLPCamp3/course-info/blob/master/%E8%AF%BE%E4%BB%B6/20190804_bert.pptx)|||
| 8.11  |  [直播]  LDA1 |[课程资料](http://47.94.6.102/NLPCamp3/Lesson-LDA1) ||||
| 8.11  |  [Review] Bert(2) |[课件](http://47.94.6.102/NLPCamp3/course-info/blob/master/%E8%AF%BE%E4%BB%B6/Review%20Session_20190811_bert2-1.pptx)|||
| 8.18  |  [Review] Mass |[课程资料](http://47.94.6.102/NLPCamp3/course-info/blob/master/%E8%AF%BE%E4%BB%B6/20190818_MASS.pptx)|||
| 8.22  |  [直播]  Collapsed Gibbs Sampler | [课程资料](http://47.94.6.102/NLPCamp3/Lesson0822)||||
| 8.24  |  [直播]  Variational Inference|[课程资料](http://47.94.6.102/NLPCamp3/Lesson0824)||||
| 8.25  |  [Review]  Mass模型实战 |[课件代码](http://47.94.6.102/NLPCamp3/course-info/blob/master/%E8%AF%BE%E4%BB%B6/MASS-github-20180825%20review%20session.zip)||||
| 8.25  |  [直播]  XLNet(2) + Chatbot |[课程资料](http://47.94.6.102/NLPCamp3/Lesson0825)||||
| 9.1   |  [Review]  xlnet模型实战 |[课件代码](http://47.94.6.102/NLPCamp3/course-info/blob/master/%E8%AF%BE%E4%BB%B6/xlnet-master-20180901%20review%20session.zip)||||

#### 课程录播和Review Session的课件链接:  https://pan.baidu.com/s/1BMXG4xY4MF3pdNZCfnPwlA 提取码: a8xr


## 答疑时间安排
|    <b>周一(全天)<b>  |    <b>周二(全天)<b>   |  <b> 周三(全天)<b>   |  <b>周四(全天)<b>   |  <b>周五(全天)<b> | <b>周六(全天)<b> | <b>周日（全天）<b>  |
|---------|---------|---------|---------|---------|---------|---------|
|    陈老师          |    陈老师           |   陈老师            |  陈老师           | 闫老师  | 闫老师             | 闫老师               |




